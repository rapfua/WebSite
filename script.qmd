---
title: "INDY Notes & Simulations"
format: dashboard
server: shiny
editor:
  markdown:
    wrap: 72
bibliography: references.bib
---

::: hidden
\newcommand*{\k}{\kappa}
\newcommand*{\s}{\mathcal{S}}
\newcommand*{\e}{\varepsilon}
\newcommand*{\l}{\lambda}
\newcommand*{\dif}{\mathop{}\!\mathrm{d}}
:::

# Page Navigation

1.  Percolation demo
2.  Vincent Tassion
3.  Metric Backbone & Spectral Clustering Simulations
4.  Graph with Gaussian clusters & ABBE prediction
5.  hybrid graph μ fixed n_neighbors varying
6.  Module dependencies
7.  Proof of THM 2.1
8.  Instructions to deploy website
9.  First Passage Percolation On Inhomogeneous Random Graphs


# 1

```{python}
#| label: imports
#| context: setup
#| echo: false
#| cache: true


import shiny
import random
import matplotlib.pyplot as plt
from matplotlib.lines import Line2D
from pipe import sort


import numpy as np
from numpy.random import poisson, uniform

import scipy.sparse

from sklearn.cluster import SpectralClustering
from sklearn.metrics import adjusted_rand_score


from shiny.express import render, ui

import networkx as nx       # for get_metric_backbone_igraph
import graphlearning as gl  # for get_Gaussian_weight_matrix
import igraph as ig         # for get_metric_backbone_igraph

```

```{python}
#| label: class_Pipe
#| cache: true

class Pipe:
    def __init__(self, function):
        self.function = function

    def __ror__(self, other):
        return self.function(other)

# # EXAMPLE USAGE

# # Define custom functions to use with the pipe
# @Pipe
# def add_one(x):
#     return x + 1
# 
# @Pipe
# def square(x):
#     return x * x
# 
# result = 5 | add_one | square
# print(result)  # Output: 36

```

```{python}
#| label: class_StringIndexed3DArray

class StringIndexed3DArray:
    def __init__(self, array, dim1_labels, dim2_labels, dim3_labels):
        self.array = array
        # Create dictionaries to map strings to indices for each dimension
        self.index_map_dim1 = {label: i for i, label in enumerate(dim1_labels)}
        self.index_map_dim2 = {label: i for i, label in enumerate(dim2_labels)}
        self.index_map_dim3 = {label: i for i, label in enumerate(dim3_labels)}
        self.dim1_labels = dim1_labels
        self.dim2_labels = dim2_labels
        self.dim3_labels = dim3_labels

    def __getitem__(self, indices):
        row_label, col_label, depth_label = indices
        # Convert string labels to indices
        row = self._convert_to_index(row_label, self.index_map_dim1)
        col = self._convert_to_index(col_label, self.index_map_dim2)
        depth = self._convert_to_index(depth_label, self.index_map_dim3)
        # Access the value or slice from the array
        return self.array[row, col, depth]
    
    def _convert_to_index(self, label, index_map):
        if isinstance(label, slice):
            # If it's a slice, return the slice itself
            return slice(
                self.index_map_dim1.get(label.start, 0) if label.start else None,
                self.index_map_dim1.get(label.stop, None),
                label.step
            )
        elif isinstance(label, str):
            # If it's a string, convert it to the index using the map
            return index_map[label]
        else:
            return label  # Allow passing integers directly

    def __setitem__(self, indices, value):
        row_label, col_label, depth_label = indices
        # Convert string labels to indices
        row = self.index_map_dim1[row_label]
        col = self.index_map_dim2[col_label]
        depth = self.index_map_dim3[depth_label]
        # Set the value in the array
        self.array[row, col, depth] = value
        
        
    def __str__(self):
        result = []
        for i, dim1_label in enumerate(self.dim1_labels):
            result.append(f"\nDim1 ({dim1_label}):\n")
            for j, dim2_label in enumerate(self.dim2_labels):
                row = f"Dim2 ({dim2_label}): "
                row += " ".join(f"{self.array[i, j, k]:.2f}" for k in range(len(self.dim3_labels)))
                result.append(row)
        return "\n".join(result)


def AVG_ARI_LIST(array_3d, n_neighbors_LIST, framework_str):
    return [array_3d[n_neighbors - 3, framework_str, slice(None)].mean() for n_neighbors in n_neighbors_LIST]
  
def STD_ARI_LIST(array_3d, n_neighbors_LIST, framework_str):
    return [array_3d[n_neighbors - 3, framework_str, slice(None)].std() for n_neighbors in n_neighbors_LIST]


```

```{python}
#| label: global variables
#| cache: true
global_SEED = 42
SC = SpectralClustering(n_clusters=2, affinity='precomputed')

```

```{python}
#| label: def_get_Gaussian_weight_matrix
#| cache: true

def get_Gaussian_weight_matrix(X, n_neighbors):
    Z = gl.weightmatrix.knn(X, n_neighbors)  # Gaussian similarity measure
    A = (Z + Z.T) / 2
    return A


```

```{python}
#| label: def_get_metric_backbone_igraph
#| cache: true

def get_metric_backbone_igraph(D):
    """
     :param D: networkx distance graph (with weight and proximity edge attribute)
     :return: Networkx Metric Backbone subgraph of D
    """
    D_ig = ig.Graph.from_networkx(D)
    distances = D_ig.distances(weights='weight')

    G = nx.Graph(D)
    G.remove_edges_from([(x, y) for x, y, w in G.edges.data('weight') if w > distances[x][y]])
    return G

```

##  {.sidebar width="300px"}

```{python}
#| label: input_selects_1
#| cache: true

ui.markdown("## Percolation Demo")

ui.input_select("p", "Probability:",
                choices=[x / 100 for x in range(1, 100)],
                selected=0.5,
                width=10
)
ui.input_select("grid_size", "Number of nodes in each dimension:",
                choices=[x for x in range(2, 101)],
                selected=10
)

```

## Column

```{python}
#| label: shiny_percolation_plot
#| cache: true
@render.plot
def percolation_plot():
  
    p = float(input.p())
    grid_size = int(input.grid_size())
    G = nx.grid_2d_graph(grid_size, grid_size)
    pos = {(x, y): (x, y) for x, y in G.nodes()}
    
    # needs to be this high in code
    plt.figure(figsize=(6, 6))  
    
    for (u, v) in G.edges():
        edge_color = 'red' if random.random() < p else 'black'
        edge_width = 3 if edge_color == 'red' else 1  # Thicker for red edges
        nx.draw_networkx_edges(G, pos, edgelist=[(u, v)], edge_color=edge_color, width=edge_width)
    
    nx.draw_networkx_nodes(G, pos, node_size=0)
    
    legend_elements = [
        Line2D([0], [0], color='red', lw=3, label  ='open   & w(e) = 1'),
        Line2D([0], [0], color='black', lw=1, label='closed & w(e) = 0'),
    ]
    
    
    plt.legend(handles=legend_elements, bbox_to_anchor=(1.4, 0.96))
    plt.gca().set_aspect('equal')
    plt.axis('off')
    plt.title(f"{grid_size}x{grid_size} Grid with p = {p:.2f}", fontsize=14)
    plt.text(
      0.5,
      -0.05, 
      'Figure 1: each edge is open with probability p.',
      fontsize=12,
      ha='center',
      va='center', 
      transform=plt.gca().transAxes
    )

    # Adjust layout to ensure the caption fits within the figure area
    plt.tight_layout()

```

# 2

## Row {.tabset}

::: {.card title="Introduction"}

<h2>Bernoulli Bond Percolation on $\mathbb{Z}^d$</h2>

<h3>Setup:</h3>

-   $G = (\mathbb{Z}^d, E)$
-   $E = \{(x, y) \in \mathbb{Z}^d \times \mathbb{Z}^d : ||x - y||_1 = 1\}$

<h3>Probabilistic framework $(\Omega, \mathcal{F}, \mathbb{P_p})$</h3>

-   $\mathbb{P}_p(\omega) = p^{\sum_{e \in E} \omega(e)} (1 - p)^{\sum_{e \in E} (1 - \omega(e))}$

-   $\Omega = \{0, 1\}^E$

-   $\mathcal{F}=$ product $\sigma$-algebra

-   $\mathbb{P}_p = \text{Bernoulli}(p)^{\bigotimes E}$

<h3>Properties:</h3>

-   Let $\omega, \eta \in \{0, 1\}^E$ be two configurations. We define
    the following **ordering**: <br>

$\qquad \qquad \omega \leq \eta \quad \iff \quad \omega(e) \leq \eta(e) \quad \forall e \in E.$
<br>

-   An **event** $A \in \mathcal{F}$ is **increasing** if

    $\qquad \omega \in A \quad \& \quad \omega \leq \eta \implies \eta \in A.$
    <br>

    -   Examples:
        $\{x \xleftrightarrow{} y\} \text{ } \& \text{ } \{|C_x| \geq 10\}$
        are increasing events.

    -   Remark : $A, B$ increasing
        $\implies A \cap B \text{ } \& \text{ } A \cup B$ increasing.

    -   Nonexample: $\{|C_0| = 10\}$ is not increasing. <br> <br>

-   A **function** $f: \Omega \to \mathbb{R}$ is **increasing** if

    $\qquad \omega \leq \eta \implies f(\omega) \leq f(\eta).$ <br>

    -   Example: $f(\omega) = |C_0(\omega)|$ is an increasing function.

    -   Connection: event $A$ increasing $\iff$ function $\mathbb{1}_A$
        increasing. <br> <br>

-   Les $(\Omega, \mathcal{A}, P)$ be a probability space. <br> A map
    $X: \Omega \to \mathbb{R}$ is a **random variable** if
    $X^{-1}(B) \stackrel{\text{notation}}{=} \{X \in B\} \in \mathcal{A}$
    for all Borel sets $B \in \mathcal{B}(\mathbb{R})$. <br> In other
    words: $X$ must a measurable map from $(\Omega, \mathcal{A})$ to
    $(\mathbb{R}, \mathcal{B}(\mathbb{R}))$. Note that the probability
    measure is irrelevant here. <br> <br> Each random variable induces a
    probability measure on $\mathbb{R}$ by
    $P_X(B) = P(X \in B) = P(X^{-1}(B))$ called the **law** of $X$. <br>
    <br>

-   **Proposition:**

    1.  Let $A \in \mathcal{F}$ be an increasing event, then <br>
        $\qquad p \mapsto \mathbb{P}_p(A)$ is non-decreasing. <br> <br>
    2.  Let $f: \Omega \to \mathbb{R}$ be a measurable, increasing,
        nonnegative, bounded function. Then <br>
        $\qquad p \mapsto \mathbb{E}_p[f]$ is non-decreasing. <br> <br>

    -   Proof:

        -   

            2.  $\implies$ 1. by taking $f = \mathbb{1}_A$.

        -   Define $X_p(e) = \mathbb{1}_{\{U_e \leq p\}}$ so that <br>
            $\qquad E[f(X_p)] = E[f(\mathbb{1}_{\{U_e \leq p\}})] = p \cdot f(1) + (1 - p) \cdot f(0)$
            <br>

        -   $P(X_p(e) = 1) = p$ and $P(X_p(e) = 0) = 1 - p$ so that the
            law associated to the $X_p$s is the same as the one
            associated to $\mathbb{P}_p$. <br>

        -   Thus, the expectation of $f$ under the law of $X_p$ is the
            same as the expectation of $f$ under $\mathbb{P}_p$. Indeed,
            the change of variable formula gives <br>
            $\qquad E_P[g(Y)] = \int_{\Omega} g \circ Y dP = \int_{\mathbb{R}} g d \mu_{\text{pushforward of P with respect to Y}} = \int_{\mathbb{R}} g dP_Y$,
            where <br>

            $\qquad P_Y(B) = P(Y \in B) = P(Y^{-1}(B))$ is the law of
            $Y$. <br>

            In our case, this gives <br>

            $\qquad E[f(X_p)] = \int_{\mathbb{R}} f dP_{X_p} \quad (g \to f, \text{ } Y \to X_p)$
            which is equal to <br>

        -   In the context of the proof, we have $Y = X_p$ and $g = f$
            so that <br>
            $\qquad E[f(X_p)] = E_P[f(X_p)] = E_{\mathbb{P}_p}[f] = E_p[f]$
            <br>
:::

::: {.card title="Sheet 1"}
<h2>Exercise 1</h2>

$\{A \xleftrightarrow{} B\}$ is measurable
$\forall A, B \subset \mathbb{Z}^d$, and the function below is
measurable.

<h2>Exercise 2</h2>

The event $\{0 \xleftrightarrow{} \infty\}$ is measurable.

<h2>Exercise 3</h2>

Compute the following probabilities:
$\quad P_p(|C_0| = 0), \quad P_p(|C_0| = 1), \quad P_p(|C_0| \geq 1) \quad \& \quad P_p(|C_0| \geq 1 \big{|} |C_x| = 0) \text{ for some } x \in \mathbb{Z}^d.$

<h2>Exercise 4</h2>

On $\mathbb{Z}^2$, consider the event
$$C_{2n,n} = \{\exists \text{ open path from left to right in the box } [0, 2n] \times [0, n]\}$$

Let $q_n = 1 - P_p(C_{2n,n})$. Show that one of the following holds:

-   $\exists \varepsilon > 0$ such that $q_n \geq \varepsilon$ for all
    $n$.
-   $\exists \varepsilon > 0$ such that $q_n \leq e^{- \varepsilon n}$
    for all $n$.

Based on this result, prove that $p_c < 1$.
:::

# 3

##  {.sidebar width="300px"}

```{python}
#| label: input_selects_2
#| cache: true

input_select_width = 10

L = list(range(100, 501, 100))
L.insert(0, 50)

ui.input_select("n", "Number of nodes in each cluster:",
                choices=L,
                selected=50,
                width=input_select_width
)

ui.input_select("d", "Number of dimensions & communities:",
                choices=list((2, 3, 4)),
                selected=2,
                width=input_select_width
)

# for graph creation (& spectral clustering)
ui.input_select("n_neighbors", "Number of nearest neighbors ",
                choices=list(range(5, 21)),
                selected=10,
                width=input_select_width
)

ui.input_select("mu_x2", "Mean of the second Gaussian with respect to the x-axis:",
                choices=list(range(1, 21)),
                selected=3,
                width=input_select_width
)


ui.input_select("λ", "Intensity parameter (N_n ~ Poisson(λ * n)):",
                choices=[1],
                selected=1,
                width=input_select_width
)

ui.input_select("R_1", "Big radius for intra-community edges:",
                choices=list(range(1, 11)),
                selected=3,
                width=input_select_width
)

ui.input_select("R_2", "Small radius for inter-community edges:",
                choices=[1, 1.5, 2, 2.5, 3],
                selected=[1.5],
                width=input_select_width
)

```

```{python}
#| label: community_detection_on_Euclidean_graphs_FUNCTIONS
#| cache: true

def euclidean_distance(tuple_1, tuple_2):
    return np.linalg.norm(np.array(tuple_1) - np.array(tuple_2))


def φ(R):
    return lambda r: 1 if r <= R else 0


def f(f_r):
    return lambda G_distance, u_idx, v_idx: f_r(euclidean_distance(tuple_1=G_distance.nodes[u_idx]['pos'], tuple_2=G_distance.nodes[v_idx]['pos']))
  

def indicator(condition):
    return 1 if condition else 0
  

def make_F(f_in, f_out):
    return lambda G, u_idx, v_idx: indicator(G.nodes[u_idx]['community'] == G.nodes[v_idx]['community']) * f_in(G, u_idx, v_idx) + (1 - indicator(G.nodes[u_idx]['community'] == G.nodes[v_idx]['community'])) * f_out(G, u_idx, v_idx)


```

```{python}
#| label: inter_and_intra_community_proportion_functions
#| cache: true


def get_inter_proportion(G):
  
  nominator = 0
  
  for u, v in G.edges():
    if G.nodes[u]['community'] != G.nodes[v]['community']:
      nominator += 1
      
  denominator = G.number_of_edges()
      
  res = nominator / denominator
  
  return res


def get_intra_proportion(G):
  return 1 - get_inter_proportion(G)
```

```{python}
#| label: functions_to_produce_samples
#| cache: true

 
def produce_samples(n, d , type_samples, mu_x2=None, SEED=global_SEED):
    
    rng = np.random.default_rng(SEED)
  
    n_rows = n * d  # one row per node
    samples = np.empty((n_rows, d + 1))
    samples[:, 0] = np.arange(len(samples))
    
    
    if type_samples == "gaussian":
    
        col_slice = slice(1, samples.shape[1] + 1)
    
        idx = 0
        for last_row in range(0, n_rows, n):  # step size is n
            mean_val = mu_x2 * idx
            row_slice = slice(last_row, last_row + n)
            samples[row_slice, col_slice] = rng.multivariate_normal(
                mean=np.insert(np.zeros(d - 1), 0, mean_val), cov=np.eye(d), size=n
            )
            idx += 1
    
    
    elif type_samples == "uniform":
      
        d_root_n = n_rows ** (1 / d)
        samples[:, 1:] = d_root_n  * rng.uniform(size=(n_rows, d))  # type(.)    : np.ndarray
                                                                     # np.shape(.): (N_n, d)
      
    else:
        raise ValueError("type_samples must be either 'gaussian' or 'uniform'")
      
    
    return samples

```

```{python}
#| label: def_produce_distance_graph
#| cache: true

def produce_distance_graph(samples, n, n_communities, n_neighbors=None, framework='gaussian', SEED=global_SEED, F=None, R1=None, R2=None):
  
    rng = np.random.default_rng(SEED)
  
    G = nx.Graph()
    
    d = {int(row[0]): (row[1], row[2]) for row in samples}
    
    G.add_nodes_from(d.keys())
    nx.set_node_attributes(G, d, 'pos')
    
    n_nodes = n * n_communities
        
    if framework == 'gaussian':
        nx.set_node_attributes(G, {node: 1 if node + 1 > n else 0 for node in G.nodes}, 'community')
    
        col_slice = slice(1, samples.shape[1] + 1)
    
        W = get_Gaussian_weight_matrix(samples[:, col_slice], n_neighbors)
    
        for i in range(n_nodes):
            for j in range(i + 1, n_nodes):
                w = W[i, j]
                if w > 0:
                    G.add_edge(i, j, weight=1 / w - 1)
                
    
    elif framework == 'ABBE':
    
        community_labels = np.array(range(1, n_communities + 1))
        nx.set_node_attributes(G, {node: rng.choice(community_labels) for node in G.nodes}, 'community')
        
        
        edges_to_add = [(u_idx, v_idx) for u_idx in range(n_nodes) for v_idx in range(u_idx + 1, n_nodes) if F(G, u_idx, v_idx) == 1]
        
        G.add_edges_from(edges_to_add)
        
    elif framework == 'hybrid':
        nx.set_node_attributes(G, {node: 1 if node + 1 > n else 0 for node in G.nodes}, 'community')
        
        
 
        edges_to_add = [(u_idx, v_idx) for u_idx in range(n_nodes) for v_idx in range(u_idx + 1, n_nodes) if F(G, u_idx, v_idx) == 1]
        
        G.add_edges_from(edges_to_add)

    return G
    
```

```{python}
#| label: def_produce_patch
#| cache: true


# Helper functions to add legend
def produce_patch(color, framework='gaussian', mu_x2=None, plus_or_minus_one=None):
  if framework == 'gaussian':
    return plt.Line2D(
        [0],
        [0], 
        marker='o', 
        color='w', 
        markerfacecolor=color, 
        markersize=8, 
        label=f'X-mean: {round(mu_x2)}'
    )
  elif framework == 'ABBE':
    return plt.Line2D(
        [0],
        [0], 
        marker='o', 
        color='w', 
        markerfacecolor=color, 
        markersize=8, 
        label=f'Community label: {plus_or_minus_one}'
    )


```

```{python}
#| label: def_get_predColors_similarity_def_draw
#| cache: true


def get_predColors_similarity(samples, col_slice, n_neighbors, true_labels, b_original=True, MB=None):
  
    A = None
    
    if b_original:
        A = get_Gaussian_weight_matrix(samples[:, col_slice], n_neighbors)
    else:
        A = nx.adjacency_matrix(MB, nodelist=[i for i in range(MB.number_of_nodes())], weight='proximity')
        A = scipy.sparse.csr_matrix(A)
        
    pred_labels = SC.fit_predict(A)
    pred_colors = ['red' if label == pred_labels[0] else 'blue' for label in pred_labels]
    similarity  = adjusted_rand_score(true_labels, pred_labels)
    
    if b_original:
        print(f"Adjusted Rand Score on Original Graph: {similarity * 100}")
    else:
        print(f"Adjusted Rand Score on MB : {similarity * 100}")
        
    return pred_colors, similarity


def draw(G, MB, samples, n_neighbors, axs, n_clusters, L_idx=[0, 1], affinity='precomputed'):
  
    pos = nx.get_node_attributes(G, 'pos')  # Extract node positions
    
    true_labels = list(nx.get_node_attributes(G, 'community').values())
    true_colors = ['red' if label == true_labels[0] else 'blue' for label in true_labels]

    col_slice = slice(1, samples.shape[1] + 1)

    SC = SpectralClustering(n_clusters=n_clusters, affinity=affinity)
    
      
    pred_colors_original, similarity_original = get_predColors_similarity(
      samples,
      col_slice,
      n_neighbors,
      true_labels
    )
    
    pred_colors_mb, similarity_mb = get_predColors_similarity(
      samples=None,
      col_slice=None,
      n_neighbors=None,
      true_labels=true_labels,
      b_original=False,
      MB=MB
    )



    nx.draw(G, pos, node_color=true_colors, node_size=5, ax=axs[L_idx[0], 0], edge_color='lightgray')
    nx.draw(MB, pos, node_color=true_colors, node_size=5, ax=axs[L_idx[0], 1], edge_color='lightgray')
    
    nx.draw(G, pos, node_color=pred_colors_original, node_size=5, ax=axs[L_idx[1], 0], edge_color='lightgray')
    nx.draw(MB, pos, node_color=pred_colors_mb, node_size=5, ax=axs[L_idx[1], 1], edge_color='lightgray')
    
    return similarity_original, similarity_mb
```

## Column

```{python}
#| label: shiny_metric_backbone_and_spectral_clustering_simulations
#| cache: true
@render.plot
def normals_nNodes_dDimensions_PLOT():
    
    n           = int(input.n())
    d           = int(input.d())
    n_clusters  = d
    n_neighbors           = int(input.n_neighbors())
    mu_x2       = float(input.mu_x2())
    #n_neighbors = int(input.n_neighbors())
    # n_neighbors = k
    λ           = int(input.λ())

    R_1         = float(input.R_1())  
    R_2         = float(input.R_2())
    R_1, R_2 = max(R_1, R_2), min(R_1, R_2)
    f_in_r  = φ(R_1)
    f_out_r = φ(R_2)
    f_in  = f(f_in_r)
    f_out = f(f_out_r)
    
    F = make_F(f_in, f_out)


    # Generate samples separately
    samples_gaussian = produce_samples(n, d, type_samples="gaussian", mu_x2=mu_x2)

    # Update G_distance separately
    G_distance = produce_distance_graph(samples_gaussian, n, d, n_neighbors)
    mb_igraph = get_metric_backbone_igraph(G_distance)

    # Now handle plotting
    fig, axs = plt.subplots(4, 2, figsize=(24, 12))
    
    similarity_original, similarity_mb = draw(G_distance, mb_igraph, samples_gaussian, n_neighbors, axs, n_clusters)

    ############## ABBE ################
    
    SC = SpectralClustering(n_clusters=n_clusters, affinity='precomputed')

    samples_uniform = produce_samples(n, d, type_samples="uniform")
    G_distance_ABBE = produce_distance_graph(samples_uniform, n, d, framework='ABBE', F=F)

    col_slice = slice(1, samples_uniform.shape[1] + 1)

    W = get_Gaussian_weight_matrix(samples_uniform[:, col_slice], n_neighbors)

    edges = list(G_distance_ABBE.edges())
    weights = {(u, v): 1 / W[u, v] - 1 if W[u, v] > 0 else float('inf') for u, v in edges}
    nx.set_edge_attributes(G_distance_ABBE, weights, 'weight')

    mb_igraph_ABBE = get_metric_backbone_igraph(G_distance_ABBE)

    similarity_original_ABBE, similarity_mb_ABBE = draw(G_distance_ABBE, mb_igraph_ABBE, samples_uniform, n_neighbors, axs, n_clusters, L_idx=[2, 3])

    for i in range(4):
        for j in range(2):
            ax = axs[i, j]
            ax.set_xlabel('X-axis')
            ax.set_ylabel('Y-axis')
            ax.axis('equal')
            ax.axis('on')
            ax.tick_params(left=True, bottom=True, labelleft=True, labelbottom=True)
            if i < 2:
                ax.legend(handles=[produce_patch(color='red', framework='gaussian', mu_x2=0), produce_patch(color='blue', framework='gaussian', mu_x2=mu_x2)])
            else:
                ax.legend(handles=[produce_patch(color='red', framework='ABBE',  plus_or_minus_one=1), produce_patch(color='blue', framework='ABBE', plus_or_minus_one=-1)])


    axs[0, 0].set_title(f'Gaussian Samples with {n} nodes in each cluster, inter-proportion: {get_inter_proportion(G_distance) * 100:.2f}%')
    axs[0, 1].set_title(f'Metric Backbone, inter-proportion: {get_inter_proportion(mb_igraph) * 100:.2f}%')
    
    axs[1, 0].set_title(f'SC: Gaussian Samples with {n} nodes in each cluster, ARI: {similarity_original * 100:.2f}%')
    axs[1, 1].set_title(f'SC: Metric Backbone, ARI: {similarity_mb * 100:.2f}%')
    
    
    axs[2, 0].set_title(f'ABBE original {G_distance_ABBE.number_of_edges()} edges, inter-proportion: {get_inter_proportion(G_distance_ABBE) * 100:.2f}%')
    axs[2, 1].set_title(f'ABBE MB {mb_igraph_ABBE.number_of_edges()} edges, inter-proportion: {get_inter_proportion(mb_igraph_ABBE) * 100:.2f}%')
    axs[3, 0].set_title(f'SC: ABBE original, ARI: {similarity_original_ABBE * 100:.2f}%')
    axs[3, 1].set_title(f'SC: ABBE MB, ARI: {similarity_mb_ABBE * 100:.2f}%')
    
```

# 4

##  {.sidebar width="300px"}

```{python}
#| label: input_selects_100
#| cache: True



input_select_width = 10

L = list(range(100, 501, 100))
L.insert(0, 50)

ui.input_select("n100", "Number of nodes in each cluster:",
                choices=L,
                selected=50,
                width=input_select_width
)

ui.input_select("d100", "Number of dimensions & communities:",
                choices=list((2, 3, 4)),
                selected=2,
                width=input_select_width
)

# for graph creation (& spectral clustering)
ui.input_select("n_neighbors100", "Number of nearest neighbors ",
                choices=list(range(5, 21)),
                selected=10,
                width=input_select_width
)

# ui.input_select("n_neighbors", "Number of nearest neighbors for spectral clustering:",
#                 choices=list(range(3, 16)),
#                 selected=4,
#                 width=input_select_width
# )

ui.input_select("mu_x2100", "Mean of the second Gaussian with respect to the x-axis:",
                choices=list(range(1, 21)),
                selected=3,
                width=input_select_width
)


ui.input_select("λ100", "Intensity parameter (N_n ~ Poisson(λ * n)):",
                choices=[1],
                selected=1,
                width=input_select_width
)

ui.input_select("R_1100", "Big radius for intra-community edges:",
                choices=[round(i * 0.01, 2) for i in range(1, 201)],
                selected= 1,
                width=input_select_width
)

ui.input_select("R_2100", "Small radius for inter-community edges:",
                choices=[round(i * 0.01, 2) for i in range(1, 101)],
                selected=0.5,
                width=input_select_width
)

```

## Column

```{python}
#| label: shiny_gaussian_ABBE_hybrid_simulation
#| cache: True
@render.plot
def graph_gaussian_clusters_ABBE_prediction_PLOT():
  
    
    n           = int(input.n100())
    d           = int(input.d100())
    n_clusters  = d
    n_neighbors = int(input.n_neighbors100())
    mu_x100       = float(input.mu_x2100())
    λ           = int(input.λ100())

    F = make_F(f(φ(float(input.R_1100()))), f(φ(float(input.R_2100()))))
    
    # Generate samples separately
    samples = produce_samples(n, d, type_samples="gaussian", mu_x2=mu_x100)
    
    # Update G_distance separately
    G = produce_distance_graph(samples, n, d, framework='hybrid', F=F)

    col_slice = slice(1, samples.shape[1] + 1)

    W = get_Gaussian_weight_matrix(samples[:, col_slice], n_neighbors)

    edges = list(G.edges())
    weights = {(u, v): 1 / W[u, v] - 1 if W[u, v] > 0 else float('inf') for u, v in edges}
    nx.set_edge_attributes(G, weights, 'weight')
    
    mb_igraph = get_metric_backbone_igraph(G)
    
    fig, axs = plt.subplots(2, 2, figsize=(12, 12))
    
    similarity_original, similarity_mb = draw(
      G,
      mb_igraph,
      samples,
      n_neighbors,
      axs,
      n_clusters,
      L_idx=[0, 1]
    )


    for i in range(2):
        for j in range(2):
            ax = axs[i, j]
            ax.set_xlabel('X-axis')
            ax.set_ylabel('Y-axis')
            ax.axis('equal')
            ax.axis('on')
            ax.tick_params(left=True, bottom=True, labelleft=True, labelbottom=True)
            ax.legend(
              handles=[produce_patch(color='red', framework='gaussian', mu_x2=0),
                       produce_patch(color='blue', framework='gaussian', mu_x2=mu_x100)]
             )


    axs[0, 0].set_title(f'Gaussian Samples with {n} nodes in each cluster, inter-proportion: {get_inter_proportion(G) * 100:.2f}%')
    axs[0, 1].set_title(f'Metric Backbone, inter-proportion: {get_inter_proportion(mb_igraph) * 100:.2f}%')
    
    axs[1, 0].set_title(f'SC: Gaussian Samples with {n} nodes in each cluster, ARI: {similarity_original * 100:.2f}%')
    axs[1, 1].set_title(f'SC: Metric Backbone, ARI: {similarity_mb * 100:.2f}%')
    
```


# 5

##  {.sidebar width="300px"}

```{python}
#| label: input_selects_5
#| cache: True


input_select_width = 10

L = list(range(100, 501, 100))
L.insert(0, 50)

ui.input_select('n_simulations4', 'Number of simulations:',
                choices=[1] + list(range(10, 101, 10)),
                selected=10,
                width=input_select_width
)

ui.input_select("n4", "Number of nodes in each cluster:",
                choices=L,
                selected=100,
                width=input_select_width
)

ui.input_select("d4", "Number of dimensions & communities:",
                choices=list((2, 3, 4)),
                selected=2,
                width=input_select_width
)


ui.input_select("mu_x24", "Mean of the second Gaussian with respect to the x-axis:",
                choices=list(range(1, 21)),
                selected=3,
                width=input_select_width
)





ui.input_select("R_14", "Big radius for intra-community edges:",
                choices=[round(i * 0.01, 2) for i in range(1, 201)],
                selected= 1,
                width=input_select_width
)

ui.input_select("R_24", "Small radius for inter-community edges:",
                choices=[round(i * 0.01, 2) for i in range(1, 101)],
                selected=0.5,
                width=input_select_width
)



```

## Column


```{python}
#| label: shiny_mu_fixed_n_neighbors_varying_simulation_hybrid
#| cache: True
@render.plot
def graph_mu_fixed_n_neighbors_varying_PLOT_HYBRID():
  
    λ           = 1

    F = make_F(f(φ(float(input.R_14()))), f(φ(float(input.R_24()))))
    
  
    n_simulations = int(input.n_simulations4())
    
    n           = int(input.n4())
    d           = int(input.d4())
    n_clusters  = d
    mu_x2       = float(input.mu_x24())
    
    n_neighbors_LIST = list(range(3, 51))

    fig, axs = plt.subplots(2, 1, figsize=(6, 12))
    
    dim3_labels = [f'similarity_{i}' for i in range(n_simulations)]
    
    array_3d = StringIndexed3DArray(array=np.zeros((len(n_neighbors_LIST), 2, n_simulations)), dim1_labels=n_neighbors_LIST, dim2_labels=['ARI_original', 'ARI_MB'], dim3_labels=dim3_labels)
    
    for i in range(n_simulations):
        print()
        print('Simulation:', i + 1, 'out of', n_simulations, 'started')
        print()
        
        samples = produce_samples(n, d, type_samples="gaussian", mu_x2=mu_x2, SEED=i)
        col_slice = slice(1, samples.shape[1] + 1)
        
        
        
        for j, n_neighbors in enumerate(n_neighbors_LIST):
            # Update G_distance separately
            G = produce_distance_graph(samples, n, d, framework='hybrid', F=F)
        
            W = get_Gaussian_weight_matrix(samples[:, col_slice], n_neighbors)
        
            edges = list(G.edges())
            weights = {(u, v): 1 / W[u, v] - 1 if W[u, v] > 0 else float('inf') for u, v in edges}
            nx.set_edge_attributes(G, weights, 'weight')
            
            MB = get_metric_backbone_igraph(G)
            
            
            true_labels = list(nx.get_node_attributes(G, 'community').values())
            true_colors = ['red' if label == true_labels[0] else 'blue' for label in true_labels]

            SC = SpectralClustering(n_clusters=n_clusters, affinity='precomputed')

            A = get_Gaussian_weight_matrix(samples[:, col_slice], n_neighbors)

            pred_labels = SC.fit_predict(A)
            pred_colors = ['red' if label == pred_labels[0] else 'blue' for label in pred_labels]
            
            array_3d[n_neighbors, 'ARI_original', f'similarity_{i}'] = adjusted_rand_score(true_labels, pred_labels)
            
            
            A = nx.adjacency_matrix(MB, nodelist=[i for i in range(MB.number_of_nodes())], weight='proximity')
            A = scipy.sparse.csr_matrix(A)

            pred_labels = SC.fit_predict(A)
            pred_colors = ['red' if label == pred_labels[0] else 'blue' for label in pred_labels]
            
            array_3d[n_neighbors, 'ARI_MB', f'similarity_{i}'] = adjusted_rand_score(true_labels, pred_labels)
            
    
    
    axs[0].set_ylim(bottom=0, top=1)
    axs[0].plot(n_neighbors_LIST, AVG_ARI_LIST(array_3d, n_neighbors_LIST, 'ARI_original'))
    axs[0].errorbar(n_neighbors_LIST,  AVG_ARI_LIST(array_3d, n_neighbors_LIST, 'ARI_original'), yerr= STD_ARI_LIST(array_3d, n_neighbors_LIST, 'ARI_original'), fmt='o', label="Mean with Std Dev", alpha=0.5)
    axs[0].set_title('Original Graph')
    
    print(STD_ARI_LIST(array_3d, n_neighbors_LIST, 'ARI_MB'))
    print(array_3d)
    
    axs[1].set_ylim(0, 1)
    axs[1].plot(n_neighbors_LIST, AVG_ARI_LIST(array_3d, n_neighbors_LIST, 'ARI_MB'))
    axs[1].errorbar(n_neighbors_LIST,  AVG_ARI_LIST(array_3d, n_neighbors_LIST, 'ARI_MB'), yerr= STD_ARI_LIST(array_3d, n_neighbors_LIST, 'ARI_MB'), fmt='o', label="Mean with Std Dev", alpha=0.5)
    axs[1].set_title('Metric Backbone')
    
    for i in range(2):
        ax = axs[i]
        ax.set_xlabel('Number of nearest neighbors')
        ax.set_ylabel('ARI')
        ax.axis('on')
        ax.tick_params(left=True, bottom=True, labelleft=True, labelbottom=True)
        if i < 2:
            ax.legend(handles=[produce_patch(color='red', framework='gaussian', mu_x2=0), produce_patch(color='blue', framework='gaussian', mu_x2=mu_x2)])
        else:
            ax.legend(handles=[produce_patch(color='red', framework='ABBE',  plus_or_minus_one=1), produce_patch(color='blue', framework='ABBE', plus_or_minus_one=-1)])
    

``` 


# 6

```{mermaid}
%%| label: module_dependencies
%%| cache: true
flowchart LR
    A[clustering.py] --> B[community_experiments_plots.py]
    A --> C[community_experiments_tables.py]
    C --> D[helper_plots.py]
    E[metric_backbone.py] --> F[SSL.py]
    E --> G[graph_builder.py]
    G --> F
    G --> H[TSC.py]
    G --> B
    G --> C
    G --> D
    G --> I[datasets.py]
    I --> J[metrics.py]
    I --> F
    D --> F
    D --> H
    J --> H
    J --> B
    I --> B
    J --> C
    I --> C
    K[EffectiveResistanceSampling/Network.py] --> G
    J --> D
```

# 7

## Change of variables formula from wikipedia

$$\int_{X_2} g \text{ } d(f_*\mu) = \int_{X_1} g \circ f \text{ } d\mu$$

for

•⁠ ⁠$f: (X_1, \Sigma_1) \to (X_2, \Sigma_2)$,

•⁠ ⁠$g: (X_2, \Sigma_2) \to (\mathbb{R}, \mathcal{B}_\mathbb{R})$,

•⁠ ⁠$\mu: \Sigma_1 \to [0, \infty]$ a measure,

•⁠ ⁠$f_\mu: \Sigma_2 \to [0, \infty]$ a measure defined by
$f_\mu(A) = \mu(f^{-1}(A))$, the pushforward measure of $\mu$ with
respect to $f$.

## Application to the proof of THM 2.1

•⁠ ⁠We replace the professor's $f$ by Grimmett's $N$.

We have

•⁠ ⁠$\mu: \mathcal{F} \to [0, \infty]$ is a measure on $[0, 1]^E$.

•⁠ ⁠$X: ([0, 1]^E, \mathbb{B(\cdot)})\to (\Omega, \mathcal{F})$ meausrable
$\implies$ \* $(X_1, \Sigma_1) = ([0, 1]^E$, $\mathbb{B(\cdot)}$), \*
$(X_2, \Sigma_2) = (\Omega, \mathcal{F})$, \* $f = X$, \* $g = N$,

So that:

$$\int_{\Omega} N \text{ } d(X_*\mu) = \int_{[0, 1]^E} N \circ X \text{ } d\mu$$

Clearly,

$$E[N \circ X] = \int_{[0, 1]^E} N \circ X \text{ } d\mu.$$

Furthermore, if we consider

-   ⁠$P_p: \mathcal{F} \to [0, 1]$,
-   ⁠$A = \{w(e_1) = u_1, \ldots, w(e_k) = u_k\} \in \mathcal{F}$

we have:

•⁠
⁠$P_p(A) = p^k(1-p)^{|E| - k} = [0, p]^k \times [p, 1]^{|E| - k} = \int_{[0, p]^k \times [p, 1]^{|E| - k}} d\lambda = \int_{X^{-1}(A)} d\lambda = \mu(X^{-1}(A))= X_*(\mu)(A)$.

so that we also have

•⁠
⁠$E_p[N] = \int_{\Omega} N \text{ } dP_p = \int_{\Omega} N \text{ } d(X_*\mu)$,

allowing us to conclude that

$$E_p[N] = E[N \circ X].$$

•⁠ ⁠Note that we are able to concentrate on sets such as
$A = \{w(e_1) = u_1, \ldots, w(e_k) = u_k\}$ because the measure $P_p$
is characterized by its output on such setss.

# 8

-   When preview yields the desired result in VsCode, compress the
    directory that contains "script.qmd" & "app.py" and upload to
    RStudio in posit.cloud

-   In posit.cloud's RStudio, do: script.qmd \> Run Document (you will
    not get a good result, don't worry)

-   rsconnect add --account rfua --name rfua --token


-   rsconnect deploy shiny -n rfua . (This can take up to three minutes)

-   Possible to develop directly from posit.cloud's RStudio by doing:

    -   script.qmd \> Run Document
    -   app.py \> Run App
    -   this makes it possible to see intermediary results.

```{python}

# rsconnect add --account rfua --name rfua --token 81C1E677FB6E5544A763A83C69AF49E9 
```


# 9

## Row {.tabset}

::: {.card title="Abstract"}
- $\e$

-   Edge weights represent the time it takes to pass through the edge.

-   For each vertex v, we want to find the path with the minimum time to
    get from the source to v.

-   **First** emphasizes the earliest possible arrival of the fluid with
    respect to the other available paths from the source.

<!-- #### The Model -->

-   Each vertex has a type from a type space. In our case, the type is
    given by the node's community.

-   **Edge probabilities** are independent of each other.

    -   Each of their probability distributions is given by an
        exponential whose parameter is a function of the two end
        vertices' community memberships.

-   Average \# of neighbors: $\tilde\lambda_n$.

-   Flooding time from a vertex v: $F(v):=\max_w d(v, w)$

-   Diameter $:= \max_v F(v)$

-   **Edge weights** $\overset{iid}{\sim} Exp(1)$

    -   These should be interpreted as the **cost** of transport over
        each edge.

    -   After all, transport does require **time AND money**.
:::

::: {.card title="convergence"}
-   $(X_n) \xrightarrow{a.s.} X \iff$ for almost all
    $\omega \in \Omega, X_n(w) \rightarrow X(\omega)$.

-   $(X_n) \xrightarrow{p} X \iff \forall \varepsilon > 0, \lim_{n \to \infty} P(|X_n - X| > \varepsilon) = 0.$

-   For
    $p > 0, (X_n) \xrightarrow{L_p} X \iff \lim_{n \to \infty} E(|X_n - X|^p) = 0.$

-   $(X_n) \xrightarrow{d} X \iff \forall x$ for which $F_X$ is
    continuous, we have $\lim_{n \to \infty} F_{X_n}(x) = F_X(x).$

-   $L_p \implies p \implies d.$

-   $a.s. \implies p.$

-   $p \implies \exists \varphi: \mathbf{N} \to \mathbf{N}$ strictly
    increasing $\: s.t. (X_{\varphi(n)}) \xrightarrow{a.s.} X.$

-   $c$ constant $+$
    $X_n \xrightarrow{d} c \implies X_n \xrightarrow{p} X.$

-   **Weak Law of Large Numbers:**
    $\overline{X}_n \xrightarrow{p} E(X_i)$ if the $X_i$s are i.i.d. &
    $E(X_i) < \infty$.

-   **Continuous mapping theorem:** $\forall g$ continuous,
    $X_n \xrightarrow{p} X \implies g(X_n) \xrightarrow{p} g(X).$
:::


::: {.card title="definitions"}
### 1.1 The $G(n, \k)$ Model {.tabset}
-   A **spanning tree** is a sub-graph that contains all original nodes
    and is a tree.

-   $X$ **Separable metric space** $\iff \exists D \subset X$ dense s.t.
    $\forall x \in X, \forall \varepsilon > 0, \exists d \in D$ for
    which $d(x, d) < \varepsilon.$

    -   E.g. $\mathbf{Q}$ is dense in $\mathbf{R}.$

    -   At the heart of the proof is the archimedean property of
        $\mathbf{R}$, by which one can always empty a bathtub using a
        glass.

-   $\mathcal{B} (X)$, the **Borel σ-algebra** on a topological space
    $X$ is the smallest σ-algebra that contains all the open sets of
    $X$.

-   Let $\mu$ be a measure on a Borel σ-algebra. For each Borel set $B$:

    -   B is a $\mu$**-continuity set** $\iff \mu(\partial B) = 0.$

-   $\s$ separable metric space & equipped with a Borel probability
    measure $\mu \implies (\s, \mu)$ is called a **ground space**.

-   The natural of the symmetric measurable **kernel**
    $\k: \s \times \s \to \mathbf{R}_{\geq 0}$ measures the density of
    edges.

    -   In this case, "symmetric" simply means that
        $\forall u, v \in \s: \k(u, v) = \k(v, u).$

-   A kernel $\k$ on a ground space $(\s, \mu)$ is **irreducible** if:

    -   $A \subset \s$ & $\k \equiv 0$ a.e on
        $A \times (\s \setminus A) \implies \mu(A) = 0$ or
        $\mu(\s \setminus A) = 0.$

-   A kernel $\k$ on a ground space $(\s, \mu)$ is **quasi-irreducible**
    if:

    -   $\exists$ a $\mu$-continuity set $\s' \subset \s$ s.t.

        -   $\mu(\s') > 0,$

        -   $\k|_{\s' \times \s'}$ is irreducible,

        -   $\k|_{\overline{\s' \times \s'}} \equiv 0.$

-   Irreducibility of kernels insures that $\exists$ a **unique** giant
    cluster in the supercritical regime.
:::

::: {.card title="description"}
-   type of each vertex $\in \s$ separable metric space equipped with a
    Borel probability measure $\mu.$

-   $\nu_n(S) := \frac{\#\{i: \: x_i \in \s\}}{n} \xrightarrow{p} \mu(\s), \forall \: \mu\text{-continuity } S \subset \s.$

-   When all conditions are met, we get the **vertex space**
    $\nu := (\s, \mu, (x_n)).$
:::

::: {.card title="graphical"}
-   A sequence of kernels $(\k_n)$ on a vertex space $(\s, \mu, (x_n))$
    is **graphical** **with limit** $\boldsymbol{\k}$ s.t.

    -   $\k \in L^1(\s \times \s, \mu \times \mu),$

    -   $\k$ is continuous a.e.,

-   if:

    -   For a.e. $(y, z) \in \s^2$: $y_n \to y$ &
        $z_n \to z \implies \k(y_n, z_n) \to \k(y, z),$

    -   $\frac{1}{n} E_{\mu}[e(G(n, \k))] \to \int_{\s^2} \k(x, y) \text{ } d \mu(x) \text{ } d \mu(y).$

-   Note: if $\k_n \equiv \k,$ we simply say that $\k$ is graphical.
:::

::: {.card title="1.3 motivation"}
$\begin{align}
E[e(G(n, \k))|v_1, \cdots v_n]
&= \frac{1}{2} \sum_{i=1}^n E[\deg(v_i)|v_1, \cdots, v_n] \\
&= \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \frac{\k(v_i, v_j)}{n} \\
&= \frac{1}{2n} \sum_{(i,j) \in [n]^2} \k(v_i, v_j)
\end{align}$

$\begin{align}
\frac{1}{n} E[e(G(n, \k))]
&= \frac{1}{n} E[E[e(G(n, \k))|v_1, \cdots v_n]] \\
&= \frac{1}{2n^2} \sum_{(i,j) \in [n]^2} \int_{\s^2} \k(u, v) \text{ } d\mu(u) \text{ } d\mu(v) \\
&= \frac{1}{2} \int_{\s^2} \k(u, v) \text{ } d\mu(u) \text{ } d\mu(v)
\end{align}$
:::

::: {.card title="Regular"}
-   We call a kernel **regular finitary** if $\exists$ a finite
    $\mu$-continuity set partition $\s_1, \cdots, \s_n$ of $\s$ for
    which:

    -   $\forall i, j \in [n]:\k|_{\s_i \times \s_j}$ is constant.

-   It is easy to make the link between regular finitary kernels and
    finite-type kernels.
:::

::: {.card title="λ_st"}
-   $\forall s, t \in \s^2, \lambda_{\boxed{s} \: t} := k(s, t) \mu_t.$

-   Recall that $\frac{n_t}{n} \xrightarrow{p} \mu_t$.

-   Number of type t neighbors of a type s vertex
    $\sim B(\tilde{n} = n_t - \mathbf{1}_{\{s = t\}}, \: \tilde{p} = \k(s, t) /n) \implies E[X] = \tilde{n} \tilde{p} = \k(s, t) (n_t - \mathbf{1}_{\{s = t\}}) / n \xrightarrow{p} \k(s, t) \mu_t = \lambda_{\boxed{s} t}.$
:::

::: {.card title="eigenfunction"}
-   Let $v := \begin{pmatrix} \pi(s) \\ \vdots \\ \pi(s) \end{pmatrix}$,

-   let
    $\Lambda := \begin{pmatrix} \lambda_{\boxed{s} \: t} \end{pmatrix}_{s, t \in \s}$,

-   let $a^T := v^T \Lambda = a^T \Lambda$,

-   let $b^T := (1 + \tilde{\lambda}) v^T$.

-   Clearly,
    $a(t) = \sum_{s \in \s} \pi(s) \lambda_{\boxed{s} t} = \sum_{s \in \s} \pi(s) \k(s, t) \mu_t = \int_{\s} \k(s, t) \mu_t \text{ } d\pi(s)$.

-   Since $a = b$, this explains the second to last equation of page

    591. 
:::

::: {.card title="Λ matrix"}
-   $(T_{\k} f)(s_i) = \int_{\s} \k(s_i, t) f(t) \text{ } d\mu(t) = \sum_{t \in \s} \k(s_i, t) \mu(t) f(t) = \sum_{t \in \s} \lambda_{\boxed{s_i} \: t} f(t)$.

-   In other words,
    $\begin{pmatrix} (T_{\k} f)(s_1) \\ \vdots \\ (T_{\k} f)(s_n) \end{pmatrix} = \Lambda \begin{pmatrix} f(t_1) \\ \vdots \\ f(t_n) \end{pmatrix}$

-   Equivalently, $\vec{T_{\k}f} = \Lambda \vec{f}$.
:::

::: {.card title="stationary"}
We suppose:

-   $f \geq 0,$

-   $\k$ quasi-irreducible.

Then $\exists! \pi$ nontrivial left eigenfunction s.t.:

-   $\int \pi(dt) = 1$, which in turn implies:

    -   $\int_{\s} \pi(ds) \k(s, t) \mu_t = (1 + \tilde{\lambda}) \pi(t)$

We call $\pi$ the **stationary type-distribution**.

In addition, we will suppose that
$\int_{\s} \frac{\pi}{\mu} d\mu < \infty$
:::

::: {.card title="Why is κ = μ?"}
-   $(\tilde{\lambda} + 1) \pi_t = \int_{\s} \k(s, t) \mu_t \text{ } d\pi(s) = \mu_t \int_{\s} \k(s, t) \text{ } d\pi(s) \stackrel{\k \text{ sym}}{=} \mu_t \int_{\s} \k(t, s) \text{ } d\pi(s) = \mu_t \int_{\s} \k(s, t) \text{ } d\pi(s) = \mu_t (\tilde{\lambda} + 1)$.

$\begin{align}
\mu_s (1 + \tilde{\lambda})
&\stackrel{\text{1.6}}{=} \mu_s \int_{\s} \k(s, t) \text{ } d\mu(t) \\
&= \sum_{t \in \s} \k(s, t) \mu_t \mu_s\\
&\stackrel{\k \text{ sym}}{=} \sum_{t \in \s} \k(t, s) \mu_t \mu_s\\
&= \int_{\s} \k(t, s) \mu_s \text{ } d\mu(t)\\
&= (\vec{\mu}^T \Lambda)_s.
\end{align}$

This shows that $\vec{\mu}$ is a none trivial eigenvector. By uniqueness
we thus have $\vec{\mu} = \vec{\pi}$.
:::

::: {.card title="homogeneous kernel"}
-   $\k$ is **homogeneous** if
    $\int_{\s} k(s, t) d\mu(t) = 1 + \tilde{\lambda}$ for a.e.
    $s \in \s$.

-   Intuitively, this means that asymptotically, the average degree of
    vertices does not depend on the type of the vertices.
:::

::: {.card title="Miscellaneous"}
-   **Markov property**: next state depends only on the current state,
    not on how the process arrived there.
-   stochastic domination
:::

::: {.card title="Questions"}
-   Why is separability of $\s$ important?

-   What is the **main** eigenvalue?

-   What is the intuition of something that is **stationary**?

-   What is meant by the **Poissonian property** in remark 2.2. and on
    Wikipedia's poisson point process page?
:::


::: {.card title="Theorem 1.1"}

### 1.2 Main results {.tabset}

**Reminders:** - The function $f: X \to Y$ is said to be uniformly
continuous if:

-   $\forall \e > 0, \exists \delta > 0$ s.t. $\forall x_1, x_2 \in X$:

    -   $d_X(x_1, x_2) < \delta \implies d_Y(f(x_1), f(x_2)) < \e.$

**Statement:** - Let $(\s, \mu)$ be a ground space,

-   let $(\k_n)$ be a sequence of uniformly continuous graphical kernels
    with $\sup_{n \in \mathbf{N}, \: x, y \in \s} \k_n(x, y) < \infty,$

-   $\tilde{\l}_n \to \tilde{\l} < \infty$ (d'où "sparse"),

-   assumption 1.1. satisfied:

    -   $\k$ quasi-irreducible,

    -   $1 + \tilde{\l} > 1,$

    -   $\int_{\s} \frac{\pi}{\mu} d\mu = 1.$

-   $u, v \in V$ chosen uniformly at random,

-   conditional on the $u$ being connected to $v$:

$$
\left( \frac{\mathcal{H}_n(u, v) - \frac{(1 + \tilde{\l}) \log n}{\tilde{\l}}}{\sqrt{\frac{(1 + \tilde{\l}) \log n}{\tilde{\l}}}}, \mathcal{P}_n(u, v) - \frac{\log n}{\tilde{\l}}  \right) \xrightarrow{d} (Z, L),
$$

where

-   $Z \sim \mathcal{N}(0, 1),$

-   $L$ is a non-degenerate real random variable whose distribution can
    be computed using ideas from branching process theory during the
    exploration phase of the components of $G(n, \k).$
:::

::: {.card title="Theorem 1.2"}
If:

-   Theorem 1.1 (sparse setting) hypotheses,

-   $\k_n$s and $\k$ are homogeneous kernels,

-   $\tilde{\l}_n \to +\infty$

OR

-   $\k$ satisfies theorem 1.1 hypotheses,

-   $\forall n \in \mathbf{N}, \k_n = (1 + \tilde{\l}_n) \k,$

-   $\tilde{\l}_n \to +\infty$

Then:

$$
\left( \frac{\mathcal{H}_n(u, v) - \frac{(1 + \tilde{\l}_n) \log n}{\tilde{\l}_n}}{\sqrt{\log n}}, \tilde{\l}_n \mathcal{P}_n(u, v) - \log n\right) \xrightarrow{d} (Z, \tilde{L})
$$

where

-   $Z \sim \mathcal{N}(0, 1),$

-   $\tilde{L} \stackrel{d}{=} \Lambda_1 + \Lambda_2 - \Lambda_3 - c$
    with $\Lambda_i \stackrel{\textit{i.i.d.}}{\sim} \text{Gumbel}$.

    -   $c$ known and different for each of the two possible sets of
        hypotheses.
:::

::: {.card title="2. Multitype branching process"}
-   $|\{$children of $x$ in
    $S \subset \s\}| \sim \text{Poisson}(\l = \text{mean} = \int_{\s} k(x, y) d\mu(y)).$

-   Time each offspring lives
    $\stackrel{\textit{i.i.d.}}{\sim} \text{Exp}(\lambda = \frac{1}{\text{mean}} = 1).$
:::


::: {.card title="Proposition 2.1"}
### Proposition 2.1 {.tabset}

**Hypotheses:**
-   finite-type continuous-time BP

-   on the nonextinction sets


**Statement:**
-   $\frac{(S_m^1, \dots, S_m^r)}{\tilde{\l} m} \xrightarrow{a.s.} \pi$

-   $\frac{(N_m^1, \dots, N_m^r)}{m} \xrightarrow{a.s.} \pi$

-   $\frac{S_m}{m} \xrightarrow{a.s.} \tilde{\l}$




-   $\frac{S_m^i}{S_m} \xrightarrow{a.s.} \pi_s$
:::

::: {.card title="Remark 2.2."}
\![Durrett](images/durrett.jpeg)

Let us make the link between the image above from
[@durrett2019probability] p147 and this remark from the paper.

**Def:**

$$
\begin{align*}
\tilde{\mu}\colon \mathcal B(\s) &\rightarrow \bf R\\
S&\mapsto \int_S \k(s, u) \dif \mu(u).
\end{align*}
$$

**Note:** in particular
$\tilde{\mu}(\{u_i\}) =\k(s, u_i) \mu(u_i), \forall i \in [n].$
:::

::: {.card title="tableau"}
+---------------------------------+---------------------------------+
| Probability: theory and         | FPP on IHRGs                    |
| examples                        |                                 |
+:===============================:+:===============================:+
| Measure space $(S, \s, \mu)$    | \$(\s ,                         |
|                                 |                                 |
|                                 | \mathcal{B}(\s), \tilde{\mu})\$ |
+---------------------------------+---------------------------------+
| $A_i$ disjoint                  | $\{u_i\}$ disjoint              |
+---------------------------------+---------------------------------+
| $m: \s \to \bf N$, or:          | $\x                             |
|                                 | i                               |
| $m: \Omega \                    | ^                               |
| t                               | s: \mathcal B(\s) \to \bf N$,   |
| i                               | or:                             |
| mes \mathcal B(\s) \to \bf N$   |                                 |
|                                 | $\xi^s: \Omega \                |
|                                 | t                               |
|                                 | i                               |
|                                 | mes \mathcal B(\s) \to \bf N$   |
+---------------------------------+---------------------------------+
| $\mu$ mean-measure of the       | $\tilde{\mu}$ mean-measure of   |
| process, *i.e.*, $m             | the process, *i.e.*,$\xi^s(     |
| (                               | \{                              |
| A                               | u                               |
| _i) \stackrel{\mathrel{\unico   | _                               |
| d                               | i\}) \stackrel{\mathrel{\unic   |
| e                               | o                               |
| {x2AEB}}}{\sim} P(\mu(A_i)).$   | d                               |
|                                 | e{x2AEB}}}{\sim} P(\tilde{\mu   |
| Here $A_i$ is fixed so $m(A_i)$ | }                               |
| is a random variable.           | (                               |
|                                 | \                               |
| We refer to this as the         | {u_i\} =\k(s, u_i) \mu(u_i))$   |
| **Poissonian property**.        |                                 |
|                                 | Here $u_i$ is fixed so $\xi^s(  |
|                                 | \{u_i\})$ is a random variable. |
+---------------------------------+---------------------------------+
| $\mu(A_i) < \infty \implies$    | \$ \tilde{\mu}({u_i}) =\k(s, u  |
|                                 | \_ i ) \mu(u_i) \<              |
|                                 | \infty \implies\$               |
+---------------------------------+---------------------------------+
| $\forall \omega \in \Omega$ ,   | $\forall \omega \in \Omega$ ,   |
| the map below is a measure on   | the map below is a measure on   |
| $\s$.                           | $\s$.                           |
|                                 |                                 |
| $$                              | $$                              |
| \begin{align*}                  | \begin{align*}                  |
| m_{\omega                       | \xi_\omega^s\colon \mat         |
| }                               | h                               |
| \                               | c                               |
| colon \s &\rightarrow \bf N\\   | al B(\s) &\rightarrow \bf N\\   |
| S&\mapsto m(\omega, S).         | S&\mapsto \xi_(\omega, s).      |
| \end{align*}                    | \end{align*}                    |
| $$                              | $$                              |
|                                 |                                 |
| Here $\omega$ is fixed, so      | Here $\omega$ is fixed, so      |
| $m_{\omega}$ is a measure.      | $\xi_\omega^s$ is a measure.    |
+---------------------------------+---------------------------------+
| Let us now describe how one can | Let us now describe how one can |
| construct such a map $m$.       | construct such a map $\xi$.     |
|                                 |                                 |
| Since $\mu(S) < \infty,$ we can | Since \$\tilde{                 |
| define the following            | \                               |
| probability measure             | mu}mu}mu}mu}mu}mu}(\mathcal S)  |
| $\nu := \frac{\mu}{\mu(S)}.$    | = \sum\_{i=1}\^n                |
|                                 |                                 |
| We sample $N \sim P(\mu(S))$    | \k(s, u_i) \mu(u_i) \<          |
| independently of everything     | \infty\$, we can define the     |
| else.                           | following probability measure   |
|                                 | $\nu := \f                      |
| We then sample $X_1, X_2, \do   | r                               |
| t                               | a                               |
| s                               | c{\tilde\mu}{\tilde\mu(\s)}.$   |
|  \stackrel{i.i.d}{\sim} \nu$.   |                                 |
|                                 | We also sample                  |
| All this allows us to finally   | $N \sim P(\tilde\mu(\s))$       |
| define $m$ via                  | independently of everything     |
|                                 | else.                           |
| \$\$ \begin{align*}             |                                 |
| m\colon \Omega                  | We then sample $X_1, X_2, \do   |
|                                 | t                               |
| \                               | s                               |
| times \s &\rightarrow \bf N\\   |  \stackrel{i.i.d}{\sim} \nu.$   |
| (                               |                                 |
| \                               | All this allows us to finally   |
| omega, A)&\mapsto |\{j \leq N   | define $\xi$ via                |
| (                               |                                 |
| \                               | $$                              |
| omega): X_j(\omega) \in A\}|.   | \begin{align*}                  |
| \end{align*}\                   | \xi\colon \Omega \times \mat    |
| omega): X_j(\omega) \in A}\|.   | h                               |
| \\end{align\*}\                 | c                               |
| omega): X_j(\omega) \in A}\|.   | al B(\s) &\rightarrow \bf N\\   |
| \\end{align\*}\                 | (                               |
| omega): X_j(\omega) \in A}\|.   | \                               |
| \\end{align\*}\                 | omega, S)&\mapsto |\{j \leq N   |
| omega): X_j(\omega) \in A}\|.   | (                               |
| \\end{align\*} \$\$             | \                               |
|                                 | omega): X_j(\omega) \in S\}|.   |
|                                 | \end{align*}                    |
|                                 | $$                              |
+---------------------------------+---------------------------------+
|                                 | All this now allows us to make  |
|                                 | sense of the stochastic         |
|                                 | equation from Remark 2.2.:      |
|                                 |                                 |
|                                 | \$\$ A_w\^{root=s}(t) =         |
|                                 | \int\_\s\                       |
|                                 | l eft( A_w\^{root=u}(t - E_i)   |
|                                 | \bf                             |
|                                 | 1_{E_i < t} + \bf 1_{E_i > t}   |
|                                 |                                 |
|                                 | \right) \dif \xi\_\omega\^s(u)  |
|                                 | = \$\$                          |
+---------------------------------+---------------------------------+
:::

::: {.card title="Conclusion"}
All{.card title="Coupling"} this now allows us to make sense of the
stochastic equation from Remark 2.2.:

$$
A_w^{root=s}(t) = \int_\s \left( A_w^{root=u}(t - E_i) \bf 1_{E_i < t} + \bf 1_{E_i > t} \right) \dif \xi_\omega^s(u) = \sum_{u \in \s} \left( A_w^{root=u}(t - E_i) \bf 1_{E_i < t} + \bf 1_{E_i > t} \right) \xi_\omega^s(\{u\}).
$$

$\begin{align*}                                                     W^s(t) &= e^{-\tilde{\lambda} t} A^s(t) \\                            &= \int_\s e^{-\tilde{\lambda} t} e^{\tilde{\lambda} E_i}e^{-\tilde{\lambda} E_i}[A^u(t - E_i) \mathbf{1}_{\{E_i < t\}} + \mathbf{ 1}_{\{E_i>t\}}] \dif \xi_s(u) \\                                             &= \int_\s e^{-\tilde{\lambda} E_i} e^{-\tilde{\lambda} (t - E_i)}[A^u(t - E_i) \mathbf{1}_{\{E_i < t\}} + \mathbf{1}_{\{E_i>t\}}] \dif \xi_s(u) \\ &= \int_\s e^{-\tilde{\lambda} E_i} [W^u(t - E_i) \mathbf{1}_{\{E_i < t\}} + e^{-\tilde{\lambda} (t - E_i)} \mathbf{1}_{\{E_i>t\}}] \dif \xi_s(u)\end{align*}$

-   Campbell's THM $\implies$ result for Laplace functional applied to a
    Poisson point process.
:::


::: {.card title="Coupling"}
<!-- 3. Embedding the BP into the IHRG {.tabset} -->
$$\text{exploration process of the neighborhood of a vertex} \stackrel{\text{relate}}{\iff} \text{branching process}$$

**Goal of section:** **Definition of**
$\Psi_k^{\text{binomial}}(t)$**:**

**Definition of the offspring distribution** $D^{\text{binomial}}$**:**

$D^{\text{bin}|\text{type}(v)=s} \mathrel{\vcenter{:}}= \sum_{j \in [r]} \eta_{sj}, \quad \eta_{sj} \stackrel{\unicode{x2AEB}}{\sim} B(n_j - \mathbf{1}_{s=j}, \k(s, j) / n)$.

$$
\eta_{st}^n \sim \text{Bin}(n_t, \k(s, t) / n) \implies \eta_{st}^n = \sum_{i=1}^{n_t} b_{st}^i, \text{ where } \forall i: b_{st}^i \stackrel{iid}{\sim} \text{Bern}(\k(s, t) / n)
$$

Let $\xi_{st}^n \sim \text{Poi}(\frac{n_t}{n} \k(s, t))$ be the poisson
random variable coupled to $\eta_{st}^n$ ($= Y_m'$ in
[@den2012probability]).

Using the result from p.9. of [@den2012probability], we get:

$$
P(\eta_{st}^n \neq \xi_{st}^n) \leq \sum_{i=1}^{n_t} \frac{\k(s,t)^2}{n^2}= \frac{n_t \, \k(s,t)^2}{n^2} = \frac{n_t}{n} \frac{\k(s,t)^2}{n}.
$$

Let $\varepsilon > 0$,

$$
\begin{align*}
P\left( \left|\frac{n_t}{n} \frac{\k(s,t)^2}{n} - \lambda_{st} \frac{\k(s,t)}{n}\right| > \varepsilon \right) &= P\left( \left|\frac{n_t}{n} \frac{\k(s,t)^2}{n} - \mu_{t} \frac{\k(s,t)^2}{n}\right| > \varepsilon \right) \\
&= P\left( \left|\frac{n_t}{n} - \mu_{t}\right| > \frac{\varepsilon n}{\k(s,t)^2} \right) \\
&\leq P\left( \left|\frac{n_t}{n} - \mu_{t}\right| > \frac{\varepsilon}{\k(s,t)^2} \right)
\end{align*}
$$

This last expression goes to $0$ as $n \to \infty$ since we know that

$$
\frac{n_t}{n} \xrightarrow{p} \mu_{t}, \, \text{as } n \to \infty.
$$

In other words:

$$
\frac{n_t \, \k(s,t)^2}{n^2}  \xrightarrow{p} \lambda_{st} \frac{\k(s,t)}{n}
$$

$$
\frac{n_t \, \k(s,t)^2}{n^2}  - \lambda_{st} \frac{\k(s,t)}{n} = o_p(1)
$$

$$
\frac{n_t \, \k(s,t)^2}{n^2}  \xrightarrow{p} \lambda_{st} \frac{\k(s,t)}{n} (1 + o_p(1)).
$$

$$
\begin{align*}
P\left( \left|\frac{n_t}{n} \frac{\k(s,t)^2}{n} - \lambda_{st} \frac{\k(s,t)}{n} (1 + o(1))\right| > \varepsilon \right) &= P\left( \left|\frac{n_t}{n} \frac{\k(s,t)^2}{n} - \mu_{t} \frac{\k(s,t)^2}{n} (1 + o(1)) \right| > \varepsilon \right) \\
&= P\left( \left|\frac{n_t}{n} - \mu_{t} (1 + o(1))\right| > \frac{\varepsilon n}{\k(s,t)^2} \right) \\
&\stackrel{\mu_t \leq1}{\leq} P\left( \left|\frac{n_t}{n} - \mu_{t} \right| + |o(1)| > \frac{\varepsilon n}{\k(s,t)^2} \right) \\
&\leq P\left( \left|\frac{n_t}{n} - \mu_{t}\right| + |o(1)| > \frac{\varepsilon}{\k(s,t)^2} \right)
\end{align*}
$$

$$
\implies\frac{n_t \, \k(s,t)^2}{n^2}  \xrightarrow{p} \lambda_{st} \frac{\k(s,t)}{n} (1 + o(1)).
$$

$$
\begin{align*}
\sum_{s\in[r]} N_m^s \sum_{t\in[r]} \frac{\l_{st}\k(s,t)}{n}
\end{align*} 
$$
:::

::: {.card title="Max"}
Cher Maximilien,

They first write:

$\eta_{st} \sim \text{Binomial}(n_t, \k(s,t) /n)$,

$\xi_{st} \sim \text{Poisson}(\lambda_{st} = \mu_t\k(s,t))$.

They then state:

We couple $\eta_{st}$ to $\xi_{st}$ with error probability
$n_t (\k(s,t)^2 / n^2) = \lambda_{st} \k(s,t) / n(1+o(1))$.

Yesterday, you explained to me that they probably meant $o_p(1).$

However, I do not understand how they get more than:

$$
n_t (\k(s,t)^2 / n^2) =\lambda_{st} \k(s,t) / n + o_p(1)
$$

$\frac{n_t}{n} \xrightarrow{p} \mu_t < \infty \implies \frac{n_t}{n} - \mu_t = o_p(1) \implies \frac{n_t}{n} - \mu_t \stackrel{\mu_t \text{ constant}}{=} \mu_t o_p(1) \implies \frac{n_t}{n} = \mu_t(1 +  o_p(1)).$

This leads to:

$n_t \frac{\k(s,t)^2}{n^2} = \frac{n_t}{n}\frac{\k(s,t)^2}{n} = \mu_t(1 +  o_p(1))\frac{\k(s,t)^2}{n}= \mu_t \k(s,t) \frac{\k(s,t)}{n}(1+o_p(1)) = \frac{\l_{st}\k(s,t)}{n}(1+o_p(1)),$

as claimed in the article.

Let us bound the coupling error. For $t$ fixed, we have:

$$
\begin{align*}
P(\exists j \leq m, D_j^{\text{bin}} \neq D_j^{\text{Poi}}) &\leq \sum_{s=1}^{n_t} P(\text{one of the splits at a vertex of type }s\text{ did not yield the same result for } D^{\text{bin}} \text{ & } D^{\text{Poi}})\\
&\leq\sum_{s=1}^{n_t} \sum_{v\in\text{set of vertices of type }s\text{ at which a split occured}}P(\text{split at }v\text{ did not yield the same result for } D^{\text{bin}} \text{ & } D^{\text{Poi}}) \\
&=\sum_{s=1}^{n_t} N_m^s \:P(\text{split at }v\text{ did not yield the same result for } D^{\text{bin}} \text{ & } D^{\text{Poi}}) \\
&\leq\sum_{s=1}^{n_t} N_m^s \: \sum_{t=1}^{n_t} P(\eta_{st} \neq\xi_{st}) \\
&\leq\sum_{s=1}^{n_t} N_m^s \: \sum_{t=1}^{n_t} \frac{\l_{st}\k(s,t)}{n}(1+o_p(1)) \\
&=\frac{m}{n}\sum_{s,t} \frac{N_m^s}{m}\l_{st}\k(s,t)(1+o_p(1)) \\
&\leq\frac{m}{n} \max \k\sum_{s,t} \frac{N_m^s}{m}\l_{st}(1+o_p(1)) \\
&=\frac{m}{n} \max \k\sum_{s,t} \pi_s\l_{st}(1+o_p(1))^2 \\
&\stackrel{\text{Slutsky}}{=}\frac{m}{n} \max \k\sum_{s,t} \pi_s\l_{st}(1+o_p(1)) \\
\end{align*}
$$

since we know if
$E_u > t \:\:(\iff \text{node }u\text{ is still alive})$ for $t$ fixed.
:::

::: {.card title="thinning"}
In $G(m, \k)$: $\mathcal H_n(i_0, j)$.

Equivalent w.r.t. distribution to the **generation** of the first
**individual** in $th(\Psi_k^{i_0})$ to have label $j.$

In $G(m, \k)$: $\mathcal P_n(i_0, j)$.

Equivalent w.r.t. distribution to the **splitting time** of that same
individual's parent, i.e., the moment his parent died and he was born.
:::

{{< include sub_quartos/_test.qmd >}}
